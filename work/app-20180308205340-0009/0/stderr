Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/03/08 20:53:42 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 4308@onehaohdd
18/03/08 20:53:42 INFO SignalUtils: Registered signal handler for TERM
18/03/08 20:53:42 INFO SignalUtils: Registered signal handler for HUP
18/03/08 20:53:42 INFO SignalUtils: Registered signal handler for INT
18/03/08 20:53:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/03/08 20:53:44 WARN Utils: Your hostname, onehaohdd resolves to a loopback address: 127.0.1.1; using 192.168.0.112 instead (on interface wlan0)
18/03/08 20:53:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/03/08 20:53:44 INFO SecurityManager: Changing view acls to: onehao
18/03/08 20:53:44 INFO SecurityManager: Changing modify acls to: onehao
18/03/08 20:53:44 INFO SecurityManager: Changing view acls groups to: 
18/03/08 20:53:44 INFO SecurityManager: Changing modify acls groups to: 
18/03/08 20:53:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(onehao); groups with view permissions: Set(); users  with modify permissions: Set(onehao); groups with modify permissions: Set()
18/03/08 20:53:45 INFO TransportClientFactory: Successfully created connection to /192.168.0.112:52585 after 196 ms (0 ms spent in bootstraps)
18/03/08 20:53:46 INFO SecurityManager: Changing view acls to: onehao
18/03/08 20:53:46 INFO SecurityManager: Changing modify acls to: onehao
18/03/08 20:53:46 INFO SecurityManager: Changing view acls groups to: 
18/03/08 20:53:46 INFO SecurityManager: Changing modify acls groups to: 
18/03/08 20:53:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(onehao); groups with view permissions: Set(); users  with modify permissions: Set(onehao); groups with modify permissions: Set()
18/03/08 20:53:46 INFO TransportClientFactory: Successfully created connection to /192.168.0.112:52585 after 8 ms (0 ms spent in bootstraps)
18/03/08 20:53:46 INFO DiskBlockManager: Created local directory at /tmp/spark-03a3315f-7b10-415b-b73d-05272bc2f85f/executor-fc50969e-32a3-45a6-996f-439651625e7e/blockmgr-0e441f89-800e-4529-880c-a17c3e78db72
18/03/08 20:53:46 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/03/08 20:53:46 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@192.168.0.112:52585
18/03/08 20:53:46 INFO WorkerWatcher: Connecting to worker spark://Worker@192.168.0.112:60291
18/03/08 20:53:46 INFO TransportClientFactory: Successfully created connection to /192.168.0.112:60291 after 14 ms (0 ms spent in bootstraps)
18/03/08 20:53:46 INFO WorkerWatcher: Successfully connected to spark://Worker@192.168.0.112:60291
18/03/08 20:53:47 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
18/03/08 20:53:47 INFO Executor: Starting executor ID 0 on host 192.168.0.112
18/03/08 20:53:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58314.
18/03/08 20:53:47 INFO NettyBlockTransferService: Server created on 192.168.0.112:58314
18/03/08 20:53:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/03/08 20:53:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(0, 192.168.0.112, 58314, None)
18/03/08 20:53:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(0, 192.168.0.112, 58314, None)
18/03/08 20:53:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(0, 192.168.0.112, 58314, None)
18/03/08 20:53:47 INFO CoarseGrainedExecutorBackend: Got assigned task 0
18/03/08 20:53:47 INFO CoarseGrainedExecutorBackend: Got assigned task 1
18/03/08 20:53:47 INFO CoarseGrainedExecutorBackend: Got assigned task 2
18/03/08 20:53:47 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
18/03/08 20:53:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
18/03/08 20:53:47 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
18/03/08 20:53:47 INFO Executor: Fetching spark://192.168.0.112:52585/jars/data-algorithms-1.0.0.jar with timestamp 1520513619587
18/03/08 20:53:47 INFO TransportClientFactory: Successfully created connection to /192.168.0.112:52585 after 10 ms (0 ms spent in bootstraps)
18/03/08 20:53:47 INFO Utils: Fetching spark://192.168.0.112:52585/jars/data-algorithms-1.0.0.jar to /tmp/spark-03a3315f-7b10-415b-b73d-05272bc2f85f/executor-fc50969e-32a3-45a6-996f-439651625e7e/spark-8f6b561f-4450-43de-9c03-ffc8dc3f27dc/fetchFileTemp7325654487836548266.tmp
18/03/08 20:53:47 INFO Utils: Copying /tmp/spark-03a3315f-7b10-415b-b73d-05272bc2f85f/executor-fc50969e-32a3-45a6-996f-439651625e7e/spark-8f6b561f-4450-43de-9c03-ffc8dc3f27dc/7931788171520513619587_cache to /home/onehao/soft/spark-2.2.0-bin-hadoop2.7/work/app-20180308205340-0009/0/./data-algorithms-1.0.0.jar
18/03/08 20:53:47 INFO Executor: Adding file:/home/onehao/soft/spark-2.2.0-bin-hadoop2.7/work/app-20180308205340-0009/0/./data-algorithms-1.0.0.jar to class loader
18/03/08 20:53:47 INFO TorrentBroadcast: Started reading broadcast variable 1
18/03/08 20:53:47 INFO TransportClientFactory: Successfully created connection to /192.168.0.112:46853 after 3 ms (0 ms spent in bootstraps)
18/03/08 20:53:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 25.7 KB, free 912.3 MB)
18/03/08 20:53:48 INFO TorrentBroadcast: Reading broadcast variable 1 took 306 ms
18/03/08 20:53:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 71.8 KB, free 912.2 MB)
18/03/08 20:53:48 INFO HadoopRDD: Input split: hdfs://localhost:9000/data/chap03/input/file2.txt:0+32
18/03/08 20:53:48 INFO TorrentBroadcast: Started reading broadcast variable 0
18/03/08 20:53:48 INFO HadoopRDD: Input split: hdfs://localhost:9000/data/chap03/input/file1.txt:0+41
18/03/08 20:53:48 INFO HadoopRDD: Input split: hdfs://localhost:9000/data/chap03/input/file3.txt:0+27
18/03/08 20:53:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.9 KB, free 912.2 MB)
18/03/08 20:53:48 INFO TorrentBroadcast: Reading broadcast variable 0 took 47 ms
18/03/08 20:53:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 320.9 KB, free 911.9 MB)
18/03/08 20:53:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
18/03/08 20:53:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
18/03/08 20:53:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
18/03/08 20:53:51 ERROR Executor: Exception in task 2.0 in stage 0.0 (TID 2)
java.io.IOException: Mkdirs failed to create file:/output/1/_temporary/0/_temporary/attempt_20180308205344_0000_m_000002_2 (exists=false, cwd=file:/home/onehao/soft/spark-2.2.0-bin-hadoop2.7/work/app-20180308205340-0009/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)
	at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
18/03/08 20:53:51 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)
java.io.IOException: Mkdirs failed to create file:/output/1/_temporary/0/_temporary/attempt_20180308205344_0000_m_000001_1 (exists=false, cwd=file:/home/onehao/soft/spark-2.2.0-bin-hadoop2.7/work/app-20180308205340-0009/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)
	at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
18/03/08 20:53:51 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.io.IOException: Mkdirs failed to create file:/output/1/_temporary/0/_temporary/attempt_20180308205344_0000_m_000000_0 (exists=false, cwd=file:/home/onehao/soft/spark-2.2.0-bin-hadoop2.7/work/app-20180308205340-0009/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)
	at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
18/03/08 20:53:51 INFO CoarseGrainedExecutorBackend: Got assigned task 3
18/03/08 20:53:51 INFO Executor: Running task 1.1 in stage 0.0 (TID 3)
18/03/08 20:53:51 INFO CoarseGrainedExecutorBackend: Got assigned task 4
18/03/08 20:53:51 INFO Executor: Running task 0.1 in stage 0.0 (TID 4)
18/03/08 20:53:51 INFO CoarseGrainedExecutorBackend: Got assigned task 5
18/03/08 20:53:51 INFO Executor: Running task 2.1 in stage 0.0 (TID 5)
18/03/08 20:53:51 INFO HadoopRDD: Input split: hdfs://localhost:9000/data/chap03/input/file3.txt:0+27
18/03/08 20:53:51 INFO HadoopRDD: Input split: hdfs://localhost:9000/data/chap03/input/file1.txt:0+41
18/03/08 20:53:51 INFO HadoopRDD: Input split: hdfs://localhost:9000/data/chap03/input/file2.txt:0+32
18/03/08 20:53:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
18/03/08 20:53:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
18/03/08 20:53:51 ERROR Executor: Exception in task 2.1 in stage 0.0 (TID 5)
java.io.IOException: Mkdirs failed to create file:/output/1/_temporary/0/_temporary/attempt_20180308205344_0000_m_000002_5 (exists=false, cwd=file:/home/onehao/soft/spark-2.2.0-bin-hadoop2.7/work/app-20180308205340-0009/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)
	at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
18/03/08 20:53:51 ERROR Executor: Exception in task 0.1 in stage 0.0 (TID 4)
java.io.IOException: Mkdirs failed to create file:/output/1/_temporary/0/_temporary/attempt_20180308205344_0000_m_000000_4 (exists=false, cwd=file:/home/onehao/soft/spark-2.2.0-bin-hadoop2.7/work/app-20180308205340-0009/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)
	at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
18/03/08 20:53:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
18/03/08 20:53:51 ERROR Executor: Exception in task 1.1 in stage 0.0 (TID 3)
java.io.IOException: Mkdirs failed to create file:/output/1/_temporary/0/_temporary/attempt_20180308205344_0000_m_000001_3 (exists=false, cwd=file:/home/onehao/soft/spark-2.2.0-bin-hadoop2.7/work/app-20180308205340-0009/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)
	at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
18/03/08 20:53:52 INFO CoarseGrainedExecutorBackend: Got assigned task 6
18/03/08 20:53:52 INFO Executor: Running task 2.2 in stage 0.0 (TID 6)
18/03/08 20:53:52 INFO HadoopRDD: Input split: hdfs://localhost:9000/data/chap03/input/file3.txt:0+27
18/03/08 20:53:52 INFO CoarseGrainedExecutorBackend: Got assigned task 7
18/03/08 20:53:52 INFO Executor: Running task 1.2 in stage 0.0 (TID 7)
18/03/08 20:53:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
18/03/08 20:53:52 INFO CoarseGrainedExecutorBackend: Got assigned task 8
18/03/08 20:53:52 INFO Executor: Running task 0.2 in stage 0.0 (TID 8)
18/03/08 20:53:52 ERROR Executor: Exception in task 2.2 in stage 0.0 (TID 6)
java.io.IOException: Mkdirs failed to create file:/output/1/_temporary/0/_temporary/attempt_20180308205344_0000_m_000002_6 (exists=false, cwd=file:/home/onehao/soft/spark-2.2.0-bin-hadoop2.7/work/app-20180308205340-0009/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)
	at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
18/03/08 20:53:52 INFO CoarseGrainedExecutorBackend: Got assigned task 9
18/03/08 20:53:52 INFO Executor: Running task 2.3 in stage 0.0 (TID 9)
18/03/08 20:53:52 INFO HadoopRDD: Input split: hdfs://localhost:9000/data/chap03/input/file2.txt:0+32
18/03/08 20:53:52 INFO HadoopRDD: Input split: hdfs://localhost:9000/data/chap03/input/file1.txt:0+41
18/03/08 20:53:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
18/03/08 20:53:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
18/03/08 20:53:52 INFO HadoopRDD: Input split: hdfs://localhost:9000/data/chap03/input/file3.txt:0+27
18/03/08 20:53:52 ERROR Executor: Exception in task 1.2 in stage 0.0 (TID 7)
java.io.IOException: Mkdirs failed to create file:/output/1/_temporary/0/_temporary/attempt_20180308205344_0000_m_000001_7 (exists=false, cwd=file:/home/onehao/soft/spark-2.2.0-bin-hadoop2.7/work/app-20180308205340-0009/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)
	at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
18/03/08 20:53:52 ERROR Executor: Exception in task 0.2 in stage 0.0 (TID 8)
java.io.IOException: Mkdirs failed to create file:/output/1/_temporary/0/_temporary/attempt_20180308205344_0000_m_000000_8 (exists=false, cwd=file:/home/onehao/soft/spark-2.2.0-bin-hadoop2.7/work/app-20180308205340-0009/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)
	at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
18/03/08 20:53:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
18/03/08 20:53:52 ERROR Executor: Exception in task 2.3 in stage 0.0 (TID 9)
java.io.IOException: Mkdirs failed to create file:/output/1/_temporary/0/_temporary/attempt_20180308205344_0000_m_000002_9 (exists=false, cwd=file:/home/onehao/soft/spark-2.2.0-bin-hadoop2.7/work/app-20180308205340-0009/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:804)
	at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)
	at org.apache.spark.internal.io.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1133)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
18/03/08 20:53:52 INFO CoarseGrainedExecutorBackend: Got assigned task 10
18/03/08 20:53:52 INFO Executor: Running task 1.3 in stage 0.0 (TID 10)
18/03/08 20:53:52 INFO CoarseGrainedExecutorBackend: Got assigned task 11
18/03/08 20:53:52 INFO Executor: Running task 0.3 in stage 0.0 (TID 11)
18/03/08 20:53:52 INFO Executor: Executor is trying to kill task 1.3 in stage 0.0 (TID 10), reason: stage cancelled
18/03/08 20:53:52 INFO Executor: Executor is trying to kill task 0.3 in stage 0.0 (TID 11), reason: stage cancelled
18/03/08 20:53:52 INFO HadoopRDD: Input split: hdfs://localhost:9000/data/chap03/input/file2.txt:0+32
18/03/08 20:53:52 INFO HadoopRDD: Input split: hdfs://localhost:9000/data/chap03/input/file1.txt:0+41
18/03/08 20:53:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
18/03/08 20:53:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
18/03/08 20:53:52 INFO Executor: Executor interrupted and killed task 1.3 in stage 0.0 (TID 10), reason: stage cancelled
18/03/08 20:53:52 INFO Executor: Executor interrupted and killed task 0.3 in stage 0.0 (TID 11), reason: stage cancelled
18/03/08 20:53:52 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
18/03/08 20:53:52 ERROR CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
tdown
